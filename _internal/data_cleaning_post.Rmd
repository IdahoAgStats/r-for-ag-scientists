---
title: "Introduction to Data Cleaning with the Tidyverse"
author: jpiaskowski
date: 2022-10-26
categories: 
- R
- tidyverse
- data cleaning
tags: 
draft: false
summary: "Common tools and functions to help clean data sets and prepare them for analysis"
---

 
```{r setup, include=TRUE, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE, eval=TRUE, include=TRUE)
```

If your file management system looks like this (no guilt, we have all been there), this is suggestive of some rather *ad hoc* approaches to data cleaning. As you work with your data, you will discover errors (this is a universal experience). How do you fix those? 

![](phd_comics_filenames.png)
[**from PHD comics**](https://phdcomics.com/comics/archive_print.php?comicid=1323)

Newer scientists are likely to use point-and-click software like spreadsheet. The problem with this approach is that there is no record of your exact action, so you save a new copy of the data set with a new name. And on it goes. There are other ways to fix data errors using a transparent process that can avoid these multiplying data sets, or clarify what distinguishes one data set from another. 

## Introduction

> Equip everyone regardless of means, to participate in the global economy that rewards data literacy.    
<center> â€”RStudio mission statement </center>

<br>

The [Tidyverse](https://www.tidyverse.org/) is a collection of packages designed to help data users work more efficiently with their data. At the time of writing this, the Tidyverse consists of over 25 packages with diverse that range from connecting to google sheets to working with databases. 

Most Tidyverse functions have equivalent functions in base R (that is, the set of function that load automaticallty when R is launched). The difference is that tidyverse functions are often faster, the syntax may be clearer and easier to use (no guarantee, tho), and most importantly, *they all follow a similar structure and the results are consistent across packages* (within limits, this depends on the nature of the function).  

**Technical note:** the Tidyverse uses "non-standard evaluation" ([NSE](http://adv-r.had.co.nz/Computing-on-the-language.html)) principles in its functions. NSE is a complicated topic, but in brief, it means that tidyverse function follow different rules than base R function. Tidyverse functions are easier to use, but, if you are used to base R functions, it will take some effort to reorient your R programming brain. Just a friendly warning before we dive in! 

## Tidyverse Packages

### Abbreviated list of packages useful for data cleaning   

As mentioned, the Tidyverse is a collection of [many packages](https://www.tidyverse.org/packages/). Below is a table of many very useful packages for data cleaning. This tutorial does not address all these packages, but they are worth checking out, depending on your data analytic needs. 

| Package | Purpose |
|------------------|-----------------------------------------------|
|dplyr | main package for organizing data |
|forcats | for managing categorical data |
|janitor | also not part of the tidyverse, has useful data cleaning functions |
|lubridate | for manipulating dates |
|purrr | advanced broadcasting of functions across data |
|readr | for reading in tabular data |
|readxl | for reading in MS excel sheets  |
|stringr | for managing "strings" (character variables) |
|tidyr | to reshape data and handle missing values |

![](tidyverse_hex.png)
**Honorable mention:** data.table, which provides advanced handling functions for data.frames. While not part of the tidyverse officially, it is very efficient. 

### Packages Used in This Tutorial

```{r}
library(readr)
library(data.table)
library(dplyr)
library(tidyr)
```

The warning messages indicate when there are identical function names across different packages. The last package loaded will override functions from previous-loaded packages when such conflicts arise. So, the order of how packages are loaded can matter. However, any function can be accessed by add a prefix of the package name and two colons to a particular function:
```
base::intersect(...)
```

## Load Data

### `read_csv()` v `read.csv()`

How do the functions `read.csv()` (from base R) and `read_csv` (from readr) differ in how they read in data? 

For this tutorial, 3 data sets will be used, "nass_cotton", "nass_barley", and "nass_barley". These are historical data sets from the National Agricultural Statistics Service that indicate the total acreage and yield for the crops indicated for each each U.S. state and year from 1866 to 2011. They can be accessed with the package 'agridat'. 

These data sets were originally obtained as thus from Agridat and saved to separate files for this tutorial. 
```{r}
library(agridat) # a package of agricultural data sets
data("nass.corn")
data("nass.barley")
data("nass.cotton")

write.csv(nass.corn, "nass_corn.csv", row.names = F)
write.csv(nass.barley, "nass_barley.csv", row.names = F)
write.csv(nass.cotton, "nass_cotton.csv", row.names = F)
```

When using the RStudio's point-and-click interface to load tabular data, it will typically call the function `read_csv()` for CSV files and `read_delim()` for text files. `read_csv()` is a wrapper for `read_delim()` with different default arguments. 

These arguments can be called using the command line interface as well. 
```{r}
nass_barley <- read.csv("nass_barley.csv") 
nass_cotton <- read_csv("nass_cotton.csv")

# contrast functions read.csv and read_csv
str(nass_barley)
str(nass_cotton)
```

### A Bit on the Tibble

A tibble is an object type created for the Tidyverse. This is the object type imported by 'readr' and returned by many tidyverse functions. The tibble is like a data frame with a few other rules. Tibbles have a simplified printing function if you accidentally type in the object name - it will only print the first 10 lines and will truncate the number of columns to fit your screen. If you type the object name of a traditional data frame, the first 1000 lines of data are printed! 

To create a new tibble, you can use the function `tribble()` to create a tibble by rows, or the function `tibble()` for building a tibble column-wise, or `as.tibble()` to coerce an object to a tibble.  

```{r, echo=FALSE,out.width="40%",out.height = "40%", fig.align='center'}
knitr::include_graphics(c("tribbles.jpg"))
```

There are a few aspects of tibbles that are fundamentally different from a standard data frame. Tibbles do not handle row names well and many tidyverse functions discard that information. This is often just fine - row names are computationally expensive to set and maintain. Also, it can be difficult to filter and sort data based on row names. However, many other (non-tidyverse) package functions rely on the `rownames` attribute, so be aware of this behavior when using tidyverse functions on data sets. 

### Function details

  * the most noticeable difference between `read.csv()` and `read_csv()` is that `read.csv()`'s default behavior is to replace all spaces and special characters in column headers with a ".", while `read_csv()` keeps all that information. Additionally, `read_csv()` tends to handle dates a bit better than `read.csv()`. 
  
  * `read_delim()` and its wrappers are faster than base versions (e.g. `read.csv()`), however, the difference will not be noticeable for small files (< 10 Mb) 
  
  * `read_delim()` offers additional flexibility such removing trailing and leading white space (" hello" and "hello  ") and automatically deleting empty rows (these things occur by default). 

  * specific values for missing data that are not "NA" (e.g. "-9", ".") can be set with with argument `na = ...`

  * the function can be sped up by setting values for the `col_types = ...` argument (e.g. character, numeric, date, et cetera): 
  
```{r}
nass_corn <- read_csv("nass_corn.csv", col_types = cols(col_integer(), col_character(), col_double(), col_double()))
str(nass_corn)
```

**Note:** if you have truly huge data, consider the `fread()` function in 'data.table', storing the data in database and using 'dbplyr' to access it, or using the 'arrow' package. 
  
  
## Data selection and subsetting
Using 'dplyr'

### Column selection

The `select()` function is used for column selection and has the general structure: `select(data, variables_to_select)`.

#### Select only the columns that you want

```{r}
barley_acres <- select(nass_barley, state, year, acres)
str(barley_acres)
head(barley_acres)
```

#### Select the columns you do not want:

```{r}
barley_yield <- select(nass_barley, -acres)
str(barley_yield)
barley_yield2 <- select(nass_barley, -(c(acres, yield))) # multiple columns
str(barley_yield2)
```

### Sorting data

Sort functions follow this structure: `sort(data, variables_to_sort)`

#### Sort across one column:*
```{r}
corn_year <- arrange(nass_corn, state)
head(corn_year)
```

#### Sort across several columns

```{r}
corn_year_yield <- arrange(nass_corn, year, yield)
head(corn_year_yield)
```

#### Sort in reverse order
```{r}
corn_year_yield_rev <- arrange(nass_corn, desc(year), desc(yield))
head(corn_year_yield_rev)
```

### Row selection 

Based on their position or index (where the first row = 1):
```{r}
corn_first100 <- slice(nass_corn, 100) # grab first 100 rows
```

### Filtering Data
*(actually a type of row selection)*

Filtering enables users to select all columns of data based on a condition within a row or rows. The `filter()` function has the formulation: `select(data, condition_for_filter)`.

#### Filter based on one variable and exact match

```{r}
corn_AZ <- filter(nass_corn, state == "Arizona")
head(corn_AZ)
```

#### Filter for matching several items: 

```{r}
barley_pnw <- filter(nass_barley, state %in% c("Idaho", "Oregon", "Washington"))
head(barley_pnw)
```

#### Filter by numerical cut-off: 

(first, look at summary of data)

```{r}
summary(nass_cotton$acres)
nass_cotton_high <- filter(nass_cotton, acres < 550000)
```

<span style="color:mediumblue">Logical operators in R:</span>

|symbol    | meaning  |        
|------|-------|
|<    | less than |
|<=	  | less than or equal to |
|>     | greater than |
|>=	   | greater than or equal to  |
| ==    | exactly equal to  |
|!=     | not equal to  |
|!x       | Not x  |
|x | y     | x OR y  |
|x & y     | x AND y  |


#### Combine multiple filters

*(and plot the output)*

```{r}
corn_Iowa_prewar <- filter(nass_corn, year < 1950 & year >= 1930 & state == "Iowa")
with(corn_Iowa_prewar, plot(year, yield, col = "dodgerblue", pch = 16, type = "b"))
```


## Data transformations

### The `mutate()` function to define new variables

Mutate functions have the formulation: `mutate(data, new_variable_name = data_transformation)`. New variables are declared on the left and the right provides the operations for variable creation itself. Many operations are allowed within a mutate function: 
```{r}
barley_HI <- mutate(nass_barley, 
                    harvest_index = acres/yield,
                    Year = as.factor(year),
                    Is_high = acres > median(acres),
                    crop = "barley", 
                    relative_yld = yield/mean(yield))

head(barley_HI)
```

### Renaming variables

The `rename()` function renames columns using this notation: `rename(dataframe, newname = "oldname")`:

```{r}
barley_HI <- rename(barley_HI, high_acres = "Is_high")
str(barley_HI)
```

### Splitting and Merging Columns

Combine multiple columns into one with the function `unite()`. Column data are pasted together as text, not added as numbers. The columns used to make the new variables are automatically discarded (this behavior can be stopped with the argument `remove = F`). 

`unite()` functions have this formulation: `unite(data, new_var_name, vars_to_select)`.

```{r}
cotton_nass_new <- unite(nass_cotton, "ID", state, year)
head(cotton_nass_new)
```

The Opposite function,`separate()`, has this structure:  `separate(data, col_to_split, names_of_new_cols)`  

```{r}
cotton_nass_new2 <- separate(cotton_nass_new, 
                             col = ID, 
                             into = c("STATE", "YEAR"), 
                             sep = "_", 
                             remove = F)
head(cotton_nass_new2)
```

These last two functions are often quite useful - for pulling apart complicated ID columns, or for creating a unique identifier that can later be used for merging two data sets together.  

## Merging data sets with 'dplyr'

Merging different data sets by a common variable is a major advantage to using automated software. Doing this manually in spreadsheet programs is nearly impossible to do without introducing errors. There are 6 different merging or joining functions which differ in what data they keep and what are discarded. 

### Standard join functions

For merging 2 data frames. Join functions are first and foremost defined by the rows they return. The function use a common ID, which can be automatically detected or explicitly declared, to match observations in A with observations in B and returned a merged data frame. In most cases, all columns from both data frames are returned. 

The general formulation for join functions: `type_join(x_data, y_data, ID_key_variable)`  

![](dplyr_joins.png)

<center> **Know Your Joins** </center>
<p><br></p>

The column(s) used for ID are only included once in the merged data frame, that is, the duplicated key column is dropped. However, if there are other (non-ID) columns with duplicated names across the data sets, the duplicate columns from first data set listed are given a ".x" suffix and the duplicate columns from second data set listed are given a ".y" suffix. 

**1. The `full_join()`:**   

* always the safest option for avoiding data removal (everything is returned)  

* in this example, the full join will produce matches when both the year and state of a single observation match the year and state, and non-matches are appended to the resulting object.  

* **Note:** for the rest of the joins, which employ a filtering step, I recommend you always use a single unique identifier (called a "key") for the joins. You can use multiple keys, but a single unique key is the clearest way to join data without generating duplicate observations.

* When there is no match, the rows from each data set are still returned and filled with NA for columns from the other data set. 


```{r}
# first create a unique identifier variable
cotton_nass_new <- unite(nass_cotton, "ID", state, year)
barley_nass_new <- unite(nass_barley, "ID", state, year)

#equivalent models - this is only true for full_joins
cotton_barley_all <- full_join(nass_cotton, nass_barley, by = c("year", "state"))
cotton_barley_all2 <- full_join(cotton_nass_new, barley_nass_new, by = "ID")

# compare dimensions after the joins - what is kept?
dim(cotton_barley_all)
dim(nass_cotton)
dim(nass_barley)
```


**2. The `inner_join()`:**   

This function only keeps what is common between data sets:

```{r}
cotton_barley_inner <- inner_join(cotton_nass_new, barley_nass_new, by = "ID")
dim(cotton_barley_inner)
head(cotton_barley_inner)
```
    - several column names duplicates resulted in the addition of ".x" to duplicate columns from the cotton data set and ".y" to duplicate columns from the barley data set. 
```{r}
colnames(cotton_barley_inner)[2:5] <- c("cotton_acres", "cotton_yield", "barley_acres", "barley_yield")
```


**3. The `left_join()`:**    

This function matches all rows for observations found in the tibble listed on the left:  

```{r}
cotton_barley_left <- left_join(cotton_nass_new, barley_nass_new, by = "ID")
dim(cotton_barley_left)
```


**4. The `right_join()`:**  


This function matches all rows for observations found in the tibble listed on the left:  

```{r}
cotton_barley_right <- right_join(cotton_nass_new, barley_nass_new, by = "ID")
dim(cotton_barley_right)
```

**5. The `semi_join()`:**   

This function is like `inner_join()` (only rows with keys in both data sets are kept) except it does not keep any data from the data set listed second:

```{r}
cotton_barley_semi <- semi_join(cotton_nass_new, barley_nass_new, by = "ID")
dim(cotton_barley_semi)
str(cotton_barley_semi)
str(cotton_barley_inner)
```

**6. The `anti_join()`:**  

Observations from first data set that *do not* match the second data set are kept:

```{r}
cotton_barley_anti <- anti_join(cotton_nass_new, barley_nass_new, by = "ID")
dim(cotton_barley_anti)
```


**Question:** What happens when there are multiple matches for an identifier when using `full_join()`, `left_join()` or `right_join()`? 

<br>

*Answer: Each time there is a new match between the keys of two data frames, a new row is added for the matched data - effectively all pairwise matches are added to the data set. There may be circumstances when this is advantageous, but usually, it is an unexpected surprise when a `full_join()` results in new data set several magnitudes larger than the two data sets combined.*


### Set operations

These are different from standard joins. They compare across *vectors* (not data.frames/tibbles) and return what has been specified in the statement. *Duplicates are always discarded.* The purpose of set operations is to determine common or uncommon observations, not merge two different data sets. 

#### Common set operations   

![](set_operations.png)

```{r}
# states with barley data but no cotton data
setdiff(nass_barley$state, nass_cotton$test)

# states with cotton data, but no barley data
setdiff(nass_cotton$state, nass_barley$state)

# states that grow both
intersect(nass_cotton$state, nass_barley$state)
```


## Reshaping data

Occasionally, you will have data in one arrangement (e.g. wide) that needs to be transformed into the long format.  Perhaps this is needed for graphing purposes, or to compare particular data points, or because another R package requires it. Doing this manually is time consuming and error-prone. There are some function in tidyr that can do this very efficiently. 

![](wide_v_long.png)
**Same data, two formats**  

Two common approaches to reshaping data: 

1. 'tidyr' (popular, fast)
1. 'data.table' (flexible, very fast with large data sets)

|reshaping action | tidyr function | data.table function |
|----------|-------------|--------------|
|long to wide | `pivot_wider()`  | `dcast()`  |
|wide to long | `pivot_longer()`   | `melt()`  |

*Note: the tidyr functions `pivot_wider()` and `pivot_longer()` replaced `spread()` and `gather()`.*

### The tidyr approach: 

#### Reshaping from long to wide

```{r}
# use the "barley_pnw" object created earlier (barley data from ID, OR and WA)
nass_barley_pnw_wide <- pivot_wider(barley_pnw,
                                id_cols = year, 
                                names_from = state,
                                values_from = yield)

head(nass_barley_pnw_wide)
sample_n(nass_barley_pnw_wide, 5)
```
There is also a `values_fill=` argument to fill in missing values when you know what those should be. The default behavior is to fill with an NA. 

#### Reshaping multiple variables

```{r}
# use the "barley_pnw" object created earlier (barley data from ID, OR and WA)
nass_barley_pnw_wide2 <- pivot_wider(barley_pnw,
                                id_cols = year, 
                                names_from = state,
                                values_from = c(yield, acres))

sample_n(nass_barley_pnw_wide2, 5)
```

#### Reshaping from wide to long 
```{r}
nass_barley_pnw_long <- pivot_longer(nass_barley_pnw_wide,
                                     cols = 2:ncol(nass_barley_pnw_wide),
                                     names_to = "State",
                                     values_to = "Yield", 
                                     values_drop_na = T)

head(nass_barley_pnw_long)
dim(nass_barley_pnw_long)
dim(barley_pnw) # dimension of the original object
```

### The data.table approach

Thus far, the package 'data.table' has not been explored in any depth (and it still won't be for space considerations). Nevertheless, 'data.table' is useful for manipulating large tables; it is extremely efficient (even better than tidyverse functions).

#### Reshaping from long to wide: 
  -we will use the previously created data set "cotton_barley_inner" from inner join of the barley and cotton data sets. 
```{r}

# ID variable needs to be separated into State and year
cotton_barley_inner2 <- separate(cotton_barley_inner, ID, c("State", "Year"), sep = "_")
head(cotton_barley_inner2)
```

Reshape cotton acreage, using State as the ID variable: 
```{r}
cotton_acres_wide <- dcast(setDT(cotton_barley_inner2), State ~ Year, value.var = "cotton_acres")
```

#### Reshaping with multiple variables  

Multiple ID variables can be specified on the left side of the tilde and/or multiple column categories can be specified on right side. Furthermore, multiple columns (the "value.var") can be specified to fill the cells.

Reshape cotton acreage, using State as the ID variable using both cotton acres and cotton yield to fill the cells:  
```{r}
cotton_wide <- dcast(setDT(cotton_barley_inner2), State ~ Year, value.var = c("cotton_acres", "cotton_yield"))

sample(colnames(cotton_wide), 30)
```

#### Implementing a summary function in the reshape 

  - if there is more than one  observation for each cell, a summary function like mean can be used. If  function is not specified, the number of observations for that variable combination will be used.  
```{r}
cotton_wide_meanYield <- dcast(setDT(cotton_barley_inner2), cotton_acres ~ Year, value.var = "cotton_yield", fun = mean, fill = 0)

sample_n(cotton_wide_meanYield , 5)[,c(1, 100:106)]
```

#### Reshaping from wide to long

```{r}
cotton_acres_long <- melt(setDT(cotton_acres_wide), 
                          id.vars = "State", 
                          measure.vars = 2:ncol(cotton_acres_wide), 
                          variable.name = "YEAR", 
                          value.name = "ACRES")

sample_n(cotton_acres_long, 5)
```

```{r}
cotton_long <- melt(setDT(cotton_wide), 
                          id.vars = "State", 
                          measure.vars = list(2:147,148:ncol(cotton_wide)), 
                          variable.name = "YEAR", 
                          value.name = c("ACRES", "YIELD"), na.rm = T)

sample_n(cotton_long, 5)
```

## Grouping and summarising data

 - The function `group_by()` provides a handy tool for breaking data into groups based on a common factor. 
 - The function `summarise()` often follows a grouping command. It will apply a summary function to a data set or a grouped data set.
 - these all rely on the idea of piping, or chaining together operations, using the pipe operator: ` %>% `, rather than (1) a complicated nesting arrangement (see below) or (2) a large number of intermediate objects that you don't actually need. 

```{r, echo=FALSE,out.width="70%",out.height = "70%", fig.align='center'}
knitr::include_graphics(c("nest_vs_pipe.jpg"))
```


**Example:** group the cotton data set by year and output the mean yield for each year. 
```{r}
cotton_annual_mean <- group_by(nass_cotton, year) %>% summarise(mean_yield = mean(yield, na.rm = T))
with(cotton_annual_mean, plot(year, mean_yield, type = "l", col = "magenta", lwd = 1.5))
```

**Example:** Determine which states had the highest yields each year and plot highest yields over time
```{r}
corn_highest_yielders <- nass_corn %>% mutate(HI = yield/acres) %>% 
  group_by(year) %>% slice(which.max(HI)) %>% 
  select(year, state, yield, HI) %>% mutate(state = as.character(state)) 

corn_highest_yielders %>% group_by(state) %>% tally(sort = T) 

with(corn_highest_yielders, plot(year, yield, type = "l", col = "blue", lwd = 1.5, main = "harvest index upper limit"))
```

## Other Useful Functions

Not every amazing function in dplyr and tidyr are covered in this tutorial. Here are a few other functions you may find helpful when preparing and analyzing data. 


|Package | Function |  Purpose  |
|---------|-----------|------------------------------------------------------------|
| dplyr | `pull()` | extracts a single column and returns a vector | 
| dplyr |  `n()` | counts observations |
| dplyr |  `n_distinct()` | counts the number of unique observations |
| dplyr | `distinct()` | keeps only unique observations |
| dplyr | `starts_with()` | used within `select()`, for selecting columns that start with a specific string of characters |
| tidyr | `drop_na()` | remove rows with any missing data |
| tidyr | `fill()` | fills in missing values with whatever occurs before it (column-wise). Its behavior is dependent on row order |
| dplyr | `relocate()` | to move a column from one position to another (very helpful when working with a very wide data set)  
| tibble | rownames_to_column | will place rownames as the first column in a tibble or data.frame 
|tidyr | `replace_na()` | replace missing values with a user-defined value (only when `is.na(a_scalar)` evaluates to "TRUE")

## Other Resources 

* [R for data science]("https://r4ds.had.co.nz/") - *Thee Book* for working with the tidyverse 
* [RStudio Cheat sheets]("https://www.rstudio.com/resources/cheatsheets/") - very handy! 
* [Package documentation]("https://www.tidyverse.org/packages/") - always worth checking out
* Tutorials by package authors: (these can often reveal package tricks that are hard to spot in the original help files) 

```
vignette("dplyr")
```
 
---
title: "Lesson 7: Importing Data into R"
---

::: {.callout-caution collapse="true"}
## Learning Goals

At the end of this lesson, you should: 

* know how to set your working directory
* know how to specify a file path
* Be able to import CSV and Excel files into R
* understand the main arguments for importing .xlsx, .xls and .csv files

:::

## Working directory and file paths

While you can simulate data or load existing data sets associated with packages for your research, most of you will need to load pre-existing data sets from you computer, or a cloud server, some other external device. 

The first thing you need to understand is the working directory and file paths. 

When an R session is initiated, it ascertains what the root working directory is based on the default settings for your R installation and any other 

## How to Import

There are several ways to import data into R.

1. Use the "Import Dataset" tool in the Environment pane.

```{r}
knitr::include_graphics(here::here("images", "import_dataset_enviro.png"))
```

1. Use the Files pane in RStudio

```{r}
knitr::include_graphics(here::here("images", "import_dataset_file.png"))
```

Both of them open a new window that looks like this: 

```{r}
knitr::include_graphics(here::here("images", "import_dataset_window.png"))
```

:::{.callout-warning}
While these point-and-click interfaces are very convenient, they don't automatically leave a trail of breadcrumbs to help you repeat the process in the future.  *But*, they do generate R code that we can capture and reuse. They are handy shortcuts that I have found especially helpful when trying to import file formats I work with rarely.  
:::

1. Manual command line import 

Ultimately, this is how anything is imported into R. As mentioned, first two options listed above are actually tools for generating code that will import a data set through the command-line! 

There's 4 common approaches for importing data into R:

* `read.csv()`
* `read_csv()`
* `read_excel()`
* `read_delim()`

### `read.csv()`

A very commonly used funtion for reading in "comma separated values" (CSV) files. I personally like this format because it is not proprietary and is compatable across many operating systems. It also limitts all sorts of extraneous formatting that itself is a barrier to reproducible research (e.g. highlighting is discarded once a CSV file is closed). 

**Example usage:**

```{r, eval=FALSE}
mycsv1 <- read.csv("data/trial_metadata.csv")
```

```{r, echo=FALSE}
mycsv1 <- read.csv(here::here("data", "trial_metadata.csv"))
```

**Result:**
```{r}
str(mycsv1)
```

**Details:**

`read.csv()` is actually a "wrapper" for another function, `read.table()`. It has taken `read.table()` and set the default arguments to work with CSV files. `read.table()` is a more generalized form providing more flexibility. 

The default arguments include:
    * `colnames = TRUE`: the first row of data is assumed to be the column names
    * nothing in the data set will be used for rownames unless we explicitly indicate so
    * `sep = ","`: each data point is separated from another by a comma
    * a newline indicator is used to separate rows of data
    * `na.strings = c("NA", "")`: cells with a either no data ("") or an "NA" will be treated as missing. 
    * if something consists of non-numeric characters, that column vector will be treated as character and not a factor



### `read_csv()`

This function is part of **readr**. It has very similar functionality to `read.csv()`, but it parses the data a wee bit different.

**Example Usage:**


First, load the package that contains the function `read_csv()`
```{r}
library(readr)
```

```{r, eval=FALSE}
mycsv2 <- read_csv("data/trial_metadata.csv", trim_ws = TRUE)
```

```{r, echo=FALSE}
mycsv2 <- read_csv(here::here("data", "trial_metadata.csv"))
```

**Result:**
```{r}
str(mycsv2)
```

**Details:**

This function takes similar arguments to `read.csv()`, although the output is more extensive. 

* Like in `read.csv()`, the default seperator is ",", missing datea are coded as empty string `""` or `NA` and the first line is assumed to be the column header
* it does not bother with a row names attribute
* the argument `trim_ws` will remove leading and trailing whitespace for data entries. So the column header " soil pH" will become "soil pH". 
* Column are preserved more clearly than `read.csv()` (including spaces and special characters). I'm honestly not fond of this behavior and usually clean up weird column names with `janitor::clean_names()`. 

The output is largely similar, although `read_csv()` actually parses dates, unlike `read.csv()`.  


### `read_excel()`

This function will read in MS Excel files (reliably)! It is truly amazing. For many many years, it was cumbersome and/or impossible to read Excel files direclty into R. 

**Example Usage:**

Load the package that contains the function `read_excel()`
```{r}
library(readxl)
```

```{r, eval=FALSE}
myxl <- read_excel("data/field_trial_2009.xlsx", sheet = "site_02")
```


```{r, echo=FALSE}
myxl <- read_excel(here::here("data", "field_trial_2009.xlsx"), sheet = "site_02", 
                   trim_ws = TRUE, na = c("", "NA"))
```

**Results:**

```{r}
str(myxl)
```

**Details**

* By default, `read_excel()` will import the first sheet unless it named by position (e.g. 1, 2, 3) or name (like in the previous example). 
* The default argument for missing values is only an empty string `""`
* It returns results very similar to `read_csv()`. 
* There is also an argument, `range` for setting a custom range of cells to read in. 

### `read_delim()`

For reading in text files! It's pretty simple. Text files are not used terribly frequently, but I see them now and then with really huge files, such as genotyping data. 

```{r, eval=FALSE}
#mytxt <- read.delim("data/genotypic_data.txt")
```

```{r, echo=FALSE}
#mytxt <- read.delim(here::here("data", "genotypic_data.txt"))
```

**Results**

```{r}
#str(mytxt)
```

**Details**

Not much to say here - it's basically like `read.csv()`! 


:::{.callout-note}
It's useful to understand how R has read a data set into an R session. R has opened a connection to the file that you have specified, read file information into the R session using system memory (your computer's RAM), and then closed the connection. 

**This is a one-way process from your file to R**

Once a file is loaded and the connection closed, there is no longer *any link* between the object loaded into memory in R and its origin file (located on on your computer, a cloud server, etc). Any changes made to the object in R will not change the file on your computer unless you explicitly run scripts to overwrite that file. This is good thing; it gives you freedom to experiment with and manipulate an object without worrying about messing up the original file.  

We will discuss later how to export R objects to your file system when you want to capture changes made to an object.   

:::

### Troubleshooting Import errors

Things frequently go wrong when importing data. This can sometimes be corrected by changing the import arguments, but often it indicates problems with the incoming data.

Some possible errors and how to fix them:

1. **Some variables which should be numeric are characters instead.**
*At least one item contains an unexpected character that renders that observation - and the rest of the vector - as a character. This might be two decimal points, a comma, or a "O" instead of "0". If possible, manually inspect the data and correct the error. *

1. **Missing data are not coded as missing.**
*Import functions have default values for what is interpreted as missing. Check the documentation and adjust the arguments as needed to capture what code a data sets is using to indicate missing data.*

The best choice is to properly arrange your data set prior to import. [Broman & Woo (2018)](https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375989) provides some straightforward recommendations on how to manage data in spreadsheets. 

### Importing Other Data types

The instructions provided above are for importing tabular data that is generally not "big data". 

Big data is a subjective term that is system-dependent (and is rapidly changing as PC computing power and memory increases). Some personal computers can easily handle a 50 Mb file, while others cannot. If you are waiting more than 5 seconds for your data to import, then consider other options. A deep discussion about how to handle large data sets are beyond the scope of this workshop, but at the very minimum, consider the package **[data.table](https://rdatatable.gitlab.io/data.table/)** and its high-performance functions for reading and writing data, `fread()` and `fwrite()`. If your data sets are too big to load directly into R, consider **[arrow](https://arrow.apache.org/docs/r/)**.  

You may also be working with data types that are not strictly tabular, at least in the form they are stored on a computer. Here are some common non-tabular data types and packages to handle import of those.

* spatial data: **[sf](https://r-spatial.github.io/sf/)**, **[sp](https://CRAN.R-project.org/package=sp)**, **[raster](https://rspatial.github.io/raster/)**
* SAS data sets: **[haven](https://haven.tidyverse.org/)**, `haven::read_sas()`
* SPSS data sets: **[haven](https://haven.tidyverse.org/)**, `haven::read_sav()`
* tabular files on Google drive: **[googledrive](https://googledrive.tidyverse.org/)** 
* image files: **[magick](https://docs.ropensci.org/magick/articles/intro.html)**

...and so much more. 

::: {.callout-tip}
## Putting it all together


:::

